{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from ingredient_phrase_tagger.training.cli import Cli\n",
    "from ingredient_phrase_tagger.training.cli import utils as ingred_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "import string\n",
    "import math\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "# Model libraries\n",
    "from tagger_model import *\n",
    "\n",
    "# Recommendation Model\n",
    "import gensim\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity, euclidean_distances, manhattan_distances\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "from IPython.core.debugger import set_trace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects for later use\n",
    "dataPath = '../data/'\n",
    "ingred_mod_save_name = 'ingredient_model_clean_tags_crf_wordOnly'\n",
    "ingred_crf_mod = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "json_files = [os.path.join(dataPath, file) for file in os.listdir(dataPath) if\n",
    "              file.endswith('.json')]\n",
    "\n",
    "raw = pd.concat([pd.read_json(file) for file in json_files])\n",
    "raw.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Ingredients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingredient Model to Apply Named-Entity-Recognition to Ingredients to be able to pull out the actual ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ingredients(recipes_ingredients):\n",
    "    return [[ingred_utils.tokenize(ingredient) for ingredient in recipe] \n",
    "                    for recipe in recipes_ingredients]\n",
    "\n",
    "def reshape_ingredients(row):\n",
    "    \"\"\"Reformat so that instead of each row being one recipe with several \n",
    "       ingredients, each row will be one ingredient\"\"\"\n",
    "    index = [row.name] * len(row['token_ingred'])\n",
    "    return pd.Series(row['token_ingred'], index = index)\n",
    "\n",
    "def predict_ingred_ner(raw):\n",
    "    \"\"\"Predict NER ingredients\"\"\"\n",
    "    \n",
    "    # Tokenize the ingredients\n",
    "    raw['token_ingred'] = parse_ingredients(raw.ingredients)\n",
    "    \n",
    "    # Reshape ingredients for tagging\n",
    "    ingreds = []\n",
    "    for i in range(raw.shape[0]):\n",
    "        ingreds.append(reshape_ingredients(raw.iloc[i]))\n",
    "    ingred_data = pd.concat(ingreds)\n",
    "\n",
    "    # Load ingredient tagger lexicon\n",
    "    ingred_lexicon = lexiconTransformer(words_min_freq=2, unknown_tag_token='OTHER', saveNamePrefix='Ingred_mod')\n",
    "    ingred_lexicon.load_lexicon()\n",
    "\n",
    "    # Convert Ingredients from words to tokens for modeling\n",
    "    indx_ingred, _ = ingred_lexicon.transform(ingred_data, [])\n",
    "    indx_ingred = pd.Series(indx_ingred, index=ingred_data.index)\n",
    "\n",
    "    # Combine sentences and tokens into a DataFrame\n",
    "    ingred_final = pd.concat([ingred_data, indx_ingred], axis=1)\n",
    "    ingred_final.columns = ['sents', 'sent_indx']\n",
    "\n",
    "    # Ingredient parameters\n",
    "    n_word_embedding_nodes=300\n",
    "    n_tag_embedding_nodes=150\n",
    "    n_RNN_nodes=400\n",
    "    n_dense_nodes=200\n",
    "\n",
    "    ingred_mod = create_test_model(ingred_mod_save_name, ingred_lexicon, crf=ingred_crf_mod, \n",
    "                                   n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                                   n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                                   n_RNN_nodes=n_RNN_nodes, \n",
    "                                   n_dense_nodes=n_dense_nodes)\n",
    "\n",
    "    ingred_preds = predict_new_tag(ingred_mod, ingred_final, ingred_lexicon)\n",
    "    \n",
    "    ingred_final['tags'] = pd.Series(ingred_preds, index=ingred_final.index)\n",
    "    \n",
    "    return ingred_final\n",
    "#     ingred_res = pd.concat([ingred_preds, ingred_preds], axis=1)\n",
    "#     ingred_res.columns = ['sents', 'sent_indx', 'predictions']\n",
    "#     return ingred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict tags of ingredients\n",
    "# ingred_preds = predict_ingred_ner(raw)\n",
    "\n",
    "# Save model output so don't need to re-run each time\n",
    "# ingred_preds.to_pickle(os.path.join(dataPath, 'ingred_predictions.pkl'))\n",
    "\n",
    "# Load model output\n",
    "ingred_preds = pd.read_pickle(os.path.join(dataPath, 'ingred_predictions.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans({key: None for key in string.punctuation})\n",
    "\n",
    "def get_ingred(row, table=table):\n",
    "    \"\"\"Find the ingredients tagged by the model.\n",
    "    \n",
    "       If no ingredients are tagged, randomly select\n",
    "       one as long as it isn't a number.\n",
    "    \"\"\"\n",
    "    tagList = [ingred for ingred, tag in zip(row['sents'], row['tags']) if tag == 'NAME']\n",
    "    \n",
    "    if tagList == []:\n",
    "        noNums = [token for token in row['sents'] if not re.search(r'\\d', token)]\n",
    "        if noNums == []:\n",
    "            return ''\n",
    "        asSent = random.choice(noNums)\n",
    "    else:\n",
    "        asSent = ' '.join(tagList)\n",
    "    \n",
    "    removeNums = re.sub(r'\\d+', '', asSent)\n",
    "    removePunct = removeNums.translate(table)\n",
    "#     removePunct = re.sub(r'{}'.format(string.punctuation), '', removeNums)\n",
    "    removeExtraSpaces = re.sub(r'\\s+', ' ', removePunct)\n",
    "    removeBegSpace = re.sub(r'^\\s', '', removeExtraSpaces)\n",
    "    return removeBegSpace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull out the ingredients and then recombine all ingredients for \n",
    "# one recipe back into a list on one row\n",
    "ingredients = ingred_preds.apply(get_ingred, axis=1)\n",
    "ingredients = ingredients.groupby(ingredients.index).apply(lambda x: [y for y in set(x.tolist()) if y != ''])\n",
    "ingredients.name = 'clean_ingredients'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_ingreds = raw.join(ingredients)\n",
    "\n",
    "# Remove those recipes that don't have ingredients\n",
    "with_ingreds = with_ingreds[~with_ingreds.ingredients.apply(lambda x: x == [] or x is None)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ingred_len = get_max_seq_len(with_ingreds['clean_ingredients'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingred_w2v = gensim.models.Word2Vec(with_ingreds['clean_ingredients'],\n",
    "                                   size=50, min_count=1, workers=-1,\n",
    "                                   window=max_ingred_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_word_mat_to_mean_embed(word_mat, w2v):\n",
    "    \"\"\"Finds the average embedding for a list of words\"\"\"\n",
    "    dim = ingred_w2v.layer1_size\n",
    "    return [np.mean([w2v.wv.word_vec(w) for w in words if w in w2v.wv.vocab.keys()]\n",
    "                             or [np.zeros(dim)], axis=0)\n",
    "                     for words in word_mat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_ingreds['avg_ingred_embedding'] = convert_word_mat_to_mean_embed(with_ingreds.clean_ingredients, ingred_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "\n",
    "def clean_and_tokenize_directions(directions, wordnet=wordnet):\n",
    "    \"\"\"Clean up directions for a recipe by:\n",
    "    \n",
    "       1. Removing 'Photograph by... statements since these wasted text\n",
    "       2. Joining all steps into one string\n",
    "       3. Removing numbers since only interested in cooking verbs\n",
    "       4. Remove C. and F. which are Celsius and Farenheit indicators\n",
    "       5. Removing extra white space.\n",
    "    \"\"\"\n",
    "    directions = [wordnet.lemmatize(x.lower()) for x in directions if not re.search(r'^Photograph', x, re.IGNORECASE)]\n",
    "    oneText = ' '.join(directions)\n",
    "    noNumbers = re.sub(r'(\\d+)\\s?x\\s?\\d+', '', oneText)\n",
    "    noNumbers = re.sub(r'\\d+', '', noNumbers)\n",
    "    noDegrees = re.sub(r' (f|c)\\.?\\b', '', noNumbers)\n",
    "    clean_directions = re.sub(r'\\s+', ' ', noDegrees)\n",
    "    tokenized_directions = text_to_word_sequence(clean_directions)\n",
    "    return tokenized_directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_ingreds['clean_directions'] = with_ingreds['directions'].apply(clean_and_tokenize_directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_w2v = gensim.models.Word2Vec(with_ingreds['clean_directions'],\n",
    "                                 size=250, min_count=4, workers=-1,\n",
    "                                 window=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooking_verbs = ['puree', 'cover', 'crumble', 'roll', 'layer', 'saute', 'rotat', \n",
    "                 'bak', 'heat', 'blend', 'dress', 'melt', 'stir', 'trim', 'soak', \n",
    "                 'microwave', 'cook', 'wrap', 'steam', 'scrape', 'gather', \n",
    "                 'quarter', 'spray', 'reduce', 'char', 'pour', 'juice', 'crush', \n",
    "                 'wash', 'sift', 'pound', 'marinat', 'spread', 'mix', 'shred', \n",
    "                 'dice', 'brush', 'stem', 'cut', 'boil', 'grate', 'slice', 'whisk', \n",
    "                 'heat', 'grill', 'fry', 'freeze', 'stuff', 'top', 'toss', 'stew', \n",
    "                 'beat', 'swirl', 'warm', 'garnish', 'grease', 'squeeze', 'flour',\n",
    "                 'place', 'press', 'whip', 'chill', 'combine', 'add', 'use',\n",
    "                 'thread', 'arrange', 'measure', 'select', 'grind']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_out_cooking_verbs(directions, cooking_verbs=cooking_verbs):\n",
    "    return re.findall(r'{}'.format('|'.join(cooking_verbs)), ' '.join(directions))\n",
    "#     return [token for token in directions if token in cooking_verbs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_ingreds['direction_verbs'] = with_ingreds['clean_directions'].apply(pull_out_cooking_verbs)\n",
    "\n",
    "# Filter out those without any directions\n",
    "with_ingreds = with_ingreds.loc[~with_ingreds['direction_verbs'].apply(lambda x: x == [])]\n",
    "\n",
    "with_ingreds['avg_directions_embedded'] = convert_word_mat_to_mean_embed(with_ingreds['direction_verbs'], dir_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean other Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure recipe names are unique so that each name is a key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_recipe_names(names):\n",
    "    \"\"\"Replace recipe names if the names already exist\"\"\"    \n",
    "    counts = dict()\n",
    "    newNames = []\n",
    "    for name in names:\n",
    "        counts[name] = counts.get(name, 0) + 1\n",
    "        newNames.append('{} {}'.format(name, str(counts[name])))\n",
    "        \n",
    "    return pd.Series(newNames, index=names.index)\n",
    "\n",
    "with_ingreds['unique_name'] = clean_recipe_names(with_ingreds.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean total time column for use in recommendation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeDict = {'hr': 60, 'min': 1, 'day': 1440}\n",
    "def calc_time(timeText, timeDict=timeDict):\n",
    "    \"\"\"Calculate time in minutes based on text\"\"\"\n",
    "    num, time = re.search(r'(\\d+)\\s+(\\w+)', timeText).groups()\n",
    "    return timeDict[time] * int(num)\n",
    "\n",
    "def find_calc_and_sum_all_time(timeInfo):\n",
    "    \n",
    "    if isinstance(timeInfo, list):\n",
    "        if timeInfo == []:\n",
    "            return np.NaN\n",
    "        timeInfo = timeInfo[0]\n",
    "    \n",
    "    if not timeInfo:\n",
    "        return np.NaN\n",
    "\n",
    "    matches = re.findall(r'(\\d+\\s+\\w+)', timeInfo)\n",
    "    if matches:\n",
    "        return sum([calc_time(time) for time in matches])\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_ingreds['cleaned_total_time'] = with_ingreds.totalTime.apply(find_calc_and_sum_all_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8154,)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of recipes that will be deleted if total Time is missing\n",
    "with_ingreds['cleaned_total_time'].loc[(with_ingreds['cleaned_total_time'].isnull())].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8391,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count the number of recipes that will be deleted if total Time is missing\n",
    "with_ingreds['cleaned_total_time'].loc[(with_ingreds['cleaned_total_time'].isnull()) |\n",
    "                                       (with_ingreds['cleaned_total_time'] > 2880)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale time \n",
    "def scale_time_to_embeddings(totalTime):\n",
    "    \"\"\"Scale the Total Time column so that it has the \n",
    "       same Min/Max as the emeddings so that it doesn't \n",
    "       dominate the recommendations.\n",
    "       \n",
    "       Will make sure the time is logged since some recipes call\n",
    "       for days which greatly skews the recipe.\n",
    "       \"\"\"\n",
    "    minNum = min(min(with_ingreds.avg_ingred_embedding.apply(min)),\n",
    "                 min(with_ingreds.avg_directions_embedded.apply(min)))\n",
    "    maxNum = max(max(with_ingreds.avg_ingred_embedding.apply(max)),\n",
    "                 max(with_ingreds.avg_directions_embedded.apply(max)))\n",
    "    scaler = MinMaxScaler(feature_range=(minNum, maxNum))\n",
    "    return scaler.fit_transform(totalTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete off rows with missing data and recipes longer than 2 days\n",
    "finalData = with_ingreds.loc[(with_ingreds['cleaned_total_time'].notnull()) |\n",
    "                                       (with_ingreds['cleaned_total_time'] < 2880)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "finalData['logged_total_time'] = finalData['cleaned_total_time'].apply(math.log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:1: FutureWarning: reshape is deprecated and will raise in a subsequent release. Please use .values.reshape(...) instead\n",
      "  if __name__ == '__main__':\n",
      "C:\\Users\\Hamburg\\Anaconda3\\envs\\tensorflow2\\lib\\site-packages\\ipykernel\\__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "finalData['scaled_total_time'] = scale_time_to_embeddings(finalData['logged_total_time'].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "finalData.to_pickle(os.path.join(dataPath, 'final_data.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final data for model\n",
    "def create_model_data(finalData):\n",
    "    \"\"\"Prepare an array to be used for recommendation model.\n",
    "    \n",
    "       Will pull out the correct columns, turn the embedding columns\n",
    "       from one column to several columns, scale all columns to put \n",
    "       them in the same feature space.\n",
    "    \"\"\"\n",
    "    \n",
    "    modelVars = ['avg_ingred_embedding', 'avg_directions_embedded', 'scaled_total_time']\n",
    "    tmpData = finalData.loc[:, modelVars]\n",
    "    \n",
    "    avgIngredCols = ['AvgIngredEmbed{}'.format(i) for i in range(len(tmpData['avg_ingred_embedding'][0]))]\n",
    "    avgDirCols = ['AvgDirEmbed{}'.format(i) for i in range(len(tmpData['avg_directions_embedded'][0]))]\n",
    "    modelData = pd.concat([pd.DataFrame.from_records(tmpData['avg_ingred_embedding'],\n",
    "                                                     columns=avgIngredCols),\n",
    "                           pd.DataFrame.from_records(tmpData['avg_directions_embedded'],\n",
    "                                                     columns=avgDirCols),\n",
    "                           pd.DataFrame(tmpData['scaled_total_time'].tolist())], axis=1, ignore_index=True)\n",
    "    modelData.index = tmpData.index\n",
    "    return modelData\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelData = create_model_data(finalData)\n",
    "modelData = modelData.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save for later use\n",
    "modelData.to_pickle(os.path.join(dataPath, 'model_data.pkl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow2]",
   "language": "python",
   "name": "conda-env-tensorflow2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

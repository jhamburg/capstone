{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from ingredient_phrase_tagger.training.cli import Cli\n",
    "from ingredient_phrase_tagger.training.cli import utils as ingred_utils\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "# Model libraries\n",
    "from tagger_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objects for later use\n",
    "dataPath = '../data/'\n",
    "ingred_mod_save_name = 'ingredient_model_clean_tags_crf_wordOnly'\n",
    "ingred_crf_mod = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in raw data\n",
    "json_files = [os.path.join(dataPath, file) for file in os.listdir(dataPath) if\n",
    "              file.endswith('.json')]\n",
    "\n",
    "raw = pd.concat([pd.read_json(file) for file in json_files])\n",
    "raw.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw.reset_index?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ingredient Model to Apply Named-Entity-Recognition to Ingredients to be able to pull out the actual ingredients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ingredients(recipes_ingredients):\n",
    "    return [[ingred_utils.tokenize(ingredient) for ingredient in recipe] \n",
    "                    for recipe in recipes_ingredients]\n",
    "\n",
    "def reshape_ingredients(row):\n",
    "    \"\"\"Reformat so that instead of each row being one recipe with several \n",
    "       ingredients, each row will be one ingredient\"\"\"\n",
    "    index = [row.name] * len(row['token_ingred'])\n",
    "    return pd.Series(row['token_ingred'], index = index)\n",
    "\n",
    "def predict_ingred_ner(raw):\n",
    "    \"\"\"Predict NER ingredients\"\"\"\n",
    "    \n",
    "    # Tokenize the ingredients\n",
    "    raw['token_ingred'] = parse_ingredients(raw.ingredients)\n",
    "    \n",
    "    # Reshape ingredients for tagging\n",
    "    ingreds = []\n",
    "    for i in range(raw.shape[0]):\n",
    "        ingreds.append(reshape_ingredients(raw.iloc[i]))\n",
    "    ingred_data = pd.concat(ingreds)\n",
    "\n",
    "    # Load ingredient tagger lexicon\n",
    "    ingred_lexicon = lexiconTransformer(words_min_freq=2, unknown_tag_token='OTHER', saveNamePrefix='Ingred_mod')\n",
    "    ingred_lexicon.load_lexicon()\n",
    "\n",
    "    # Convert Ingredients from words to tokens for modeling\n",
    "    indx_ingred, _ = ingred_lexicon.transform(ingred_data, [])\n",
    "    indx_ingred = pd.Series(indx_ingred, index=ingred_data.index)\n",
    "\n",
    "    # Combine sentences and tokens into a DataFrame\n",
    "    ingred_final = pd.concat([ingred_data, indx_ingred], axis=1)\n",
    "    ingred_final.columns = ['sents', 'sent_indx']\n",
    "\n",
    "    # Ingredient parameters\n",
    "    n_word_embedding_nodes=300\n",
    "    n_tag_embedding_nodes=150\n",
    "    n_RNN_nodes=400\n",
    "    n_dense_nodes=200\n",
    "\n",
    "    ingred_mod = create_test_model(ingred_mod_save_name, ingred_lexicon, crf=ingred_crf_mod, \n",
    "                                   n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                                   n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                                   n_RNN_nodes=n_RNN_nodes, \n",
    "                                   n_dense_nodes=n_dense_nodes)\n",
    "\n",
    "    ingred_preds = predict_new_tag(ingred_mod, ingred_final, ingred_lexicon)\n",
    "    \n",
    "    return (ingred_final, ingred_preds)\n",
    "#     ingred_res = pd.concat([ingred_preds, ingred_preds], axis=1)\n",
    "#     ingred_res.columns = ['sents', 'sent_indx', 'predictions']\n",
    "#     return ingred_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the ingredients\n",
    "raw['token_ingred'] = parse_ingredients(raw.ingredients)\n",
    "\n",
    "# Reshape ingredients for tagging\n",
    "ingreds = []\n",
    "for i in range(raw.shape[0]):\n",
    "    ingreds.append(reshape_ingredients(raw.iloc[i]))\n",
    "ingred_data = pd.concat(ingreds)\n",
    "\n",
    "# Load ingredient tagger lexicon\n",
    "ingred_lexicon = lexiconTransformer(words_min_freq=2, unknown_tag_token='OTHER', saveNamePrefix='Ingred_mod')\n",
    "ingred_lexicon.load_lexicon()\n",
    "\n",
    "# Convert Ingredients from words to tokens for modeling\n",
    "indx_ingred, _ = ingred_lexicon.transform(ingred_data, [])\n",
    "indx_ingred = pd.Series(indx_ingred, index=ingred_data.index)\n",
    "\n",
    "# Combine sentences and tokens into a DataFrame\n",
    "ingred_final = pd.concat([ingred_data, indx_ingred], axis=1)\n",
    "ingred_final.columns = ['sents', 'sent_indx']\n",
    "\n",
    "# Get length of longest sequence\n",
    "max_seq_len = get_max_seq_len(ingred_final['sent_indx'])\n",
    "\n",
    "#Add one to max length for offsetting sequence by 1\n",
    "padded_words = pad_idx_seqs(ingred_final['sent_indx'], \n",
    "                                  max_seq_len + 1) \n",
    "\n",
    "# Ingredient parameters\n",
    "n_word_embedding_nodes=300\n",
    "n_tag_embedding_nodes=150\n",
    "n_RNN_nodes=400\n",
    "n_dense_nodes=200\n",
    "\n",
    "ingred_mod = create_test_model(ingred_mod_save_name, ingred_lexicon, crf=ingred_crf_mod, \n",
    "                               n_word_embedding_nodes=n_word_embedding_nodes,\n",
    "                               n_tag_embedding_nodes=n_tag_embedding_nodes,\n",
    "                               n_RNN_nodes=n_RNN_nodes, \n",
    "                               n_dense_nodes=n_dense_nodes,\n",
    "                              seq_input_len=max_seq_len,\n",
    "                              stateful=False)\n",
    "\n",
    "def reshape_and_predict_row(x):\n",
    "    preds = ingred_mod.predict(np.reshape(x, [1, max_seq_len]))\n",
    "    pred_tags = np.apply_along_axis(lambda x: np.argmax(x, axis=-1), 2, preds)\n",
    "    pred_tags = np.reshape(pred_tags, (max_seq_len))\n",
    "    return [ingred_lexicon.indx_to_tags_dict[tag] if tag in ingred_lexicon.indx_to_tags_dict.keys() else 0 for tag in pred_tags]\n",
    "\n",
    "preds = np.apply_along_axis(reshape_and_predict_row, 1, padded_words[:, 1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.apply_along_axis(reshape_and_predict_row, 1, padded_words[:10, 1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False,  True,  True],\n",
       "       [False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False,  True]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp == 'NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot concatenate object of type \"<class 'list'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8c55c8c0be7f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mingred_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_ingred_ner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-e2d9864d6ace>\u001b[0m in \u001b[0;36mpredict_ingred_ner\u001b[0;34m(raw)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mingred_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_new_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingred_mod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingred_final\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingred_lexicon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mingred_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mingred_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mingred_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mingred_res\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'sents'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sent_indx'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'predictions'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mingred_res\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    210\u001b[0m                        \u001b[0mkeys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                        \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                        copy=copy)\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy)\u001b[0m\n\u001b[1;32m    270\u001b[0m                        \u001b[0;34m' only pd.Series, pd.DataFrame, and pd.Panel'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                        ' (deprecated) objs are valid'.format(type(obj)))\n\u001b[0;32m--> 272\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type \"<class 'list'>\"; only pd.Series, pd.DataFrame, and pd.Panel (deprecated) objs are valid"
     ]
    }
   ],
   "source": [
    "ingred_preds = predict_ingred_ner(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingred_preds.to_pickle(os.path.join(dataPath, 'ingred_predictions.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = raw.directions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grease and flour two 8 x 4 inch pans.  Preheat oven to 325 degrees F (165 degrees C).',\n",
       " 'Sift flour, salt, baking powder, soda, and cinnamon together in a bowl.',\n",
       " 'Beat eggs, oil, vanilla, and sugar together in a large bowl.  Add sifted ingredients to the creamed mixture, and beat well.  Stir in zucchini and nuts until well combined.  Pour batter into prepared pans.',\n",
       " 'Bake for 40 to 60 minutes, or until tester inserted in the center comes out clean.  Cool in pan on rack for 20 minutes.  Remove bread from pan, and completely cool.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = [list(gensim.utils.tokenize(x, lower=True)) for x in aa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Grease and flour two 8 x 4 inch pans.  Preheat oven to 325 degrees F (165 degrees C).',\n",
       " 'Sift flour, salt, baking powder, soda, and cinnamon together in a bowl.',\n",
       " 'Beat eggs, oil, vanilla, and sugar together in a large bowl.  Add sifted ingredients to the creamed mixture, and beat well.  Stir in zucchini and nuts until well combined.  Pour batter into prepared pans.',\n",
       " 'Bake for 40 to 60 minutes, or until tester inserted in the center comes out clean.  Cool in pan on rack for 20 minutes.  Remove bread from pan, and completely cool.']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'doc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-351d2fd52962>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maa\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mpipeline.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.Tagger.__call__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpipeline.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.Tagger.predict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         '''\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_layers\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    159\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m         '''\n\u001b[1;32m--> 161\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    162\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    163\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\neural\\_classes\\model.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 125\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbegin_update\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    126\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mfeature_extracter_fwd\u001b[1;34m(docs, drop)\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_feats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint64'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfeature_extracter_bwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0md_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 337\u001b[1;33m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_feats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint64'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    338\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfeature_extracter_bwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0md_features\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\capstone\\lib\\site-packages\\thinc\\api.py\u001b[0m in \u001b[0;36mget_feats\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    334\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    335\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 336\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    337\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_feats\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'uint64'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mfeature_extracter_bwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_features\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'doc'"
     ]
    }
   ],
   "source": [
    "nlp.tagger(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('grease', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('flour', 'JJ'),\n",
       "  ('two', 'CD'),\n",
       "  ('x', 'JJ'),\n",
       "  ('inch', 'NN'),\n",
       "  ('pans', 'NNS'),\n",
       "  ('preheat', 'VBP'),\n",
       "  ('oven', 'RB'),\n",
       "  ('to', 'TO'),\n",
       "  ('degrees', 'NNS'),\n",
       "  ('f', 'VB'),\n",
       "  ('degrees', 'NNS'),\n",
       "  ('c', 'VB')],\n",
       " [('sift', 'VB'),\n",
       "  ('flour', 'JJ'),\n",
       "  ('salt', 'NN'),\n",
       "  ('baking', 'VBG'),\n",
       "  ('powder', 'NN'),\n",
       "  ('soda', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('cinnamon', 'NN'),\n",
       "  ('together', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('bowl', 'NN')],\n",
       " [('beat', 'NN'),\n",
       "  ('eggs', 'NNS'),\n",
       "  ('oil', 'NN'),\n",
       "  ('vanilla', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('sugar', 'NN'),\n",
       "  ('together', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('large', 'JJ'),\n",
       "  ('bowl', 'NN'),\n",
       "  ('add', 'NN'),\n",
       "  ('sifted', 'VBD'),\n",
       "  ('ingredients', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('the', 'DT'),\n",
       "  ('creamed', 'JJ'),\n",
       "  ('mixture', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('beat', 'NN'),\n",
       "  ('well', 'RB'),\n",
       "  ('stir', 'RB'),\n",
       "  ('in', 'IN'),\n",
       "  ('zucchini', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('nuts', 'NNS'),\n",
       "  ('until', 'IN'),\n",
       "  ('well', 'RB'),\n",
       "  ('combined', 'VBN'),\n",
       "  ('pour', 'VBP'),\n",
       "  ('batter', 'NN'),\n",
       "  ('into', 'IN'),\n",
       "  ('prepared', 'JJ'),\n",
       "  ('pans', 'NNS')],\n",
       " [('bake', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('to', 'TO'),\n",
       "  ('minutes', 'NNS'),\n",
       "  ('or', 'CC'),\n",
       "  ('until', 'IN'),\n",
       "  ('tester', 'RB'),\n",
       "  ('inserted', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('center', 'NN'),\n",
       "  ('comes', 'VBZ'),\n",
       "  ('out', 'RP'),\n",
       "  ('clean', 'JJ'),\n",
       "  ('cool', 'NN'),\n",
       "  ('in', 'IN'),\n",
       "  ('pan', 'NN'),\n",
       "  ('on', 'IN'),\n",
       "  ('rack', 'NN'),\n",
       "  ('for', 'IN'),\n",
       "  ('minutes', 'NNS'),\n",
       "  ('remove', 'VBP'),\n",
       "  ('bread', 'NN'),\n",
       "  ('from', 'IN'),\n",
       "  ('pan', 'NN'),\n",
       "  ('and', 'CC'),\n",
       "  ('completely', 'RB'),\n",
       "  ('cool', 'JJ')]]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[nltk.pos_tag(x) for x in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
